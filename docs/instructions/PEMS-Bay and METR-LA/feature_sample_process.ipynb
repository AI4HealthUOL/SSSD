{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f2fc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0f1180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pems bay example\n",
    "\n",
    "dataset = np.load('path to raw dataset after downloaded') \n",
    "dataset = dataset[50000,325] # 50000 L, 325 K\n",
    "dataset = np.array(np.split(dataset, 250, 0)) # 250 N, 200 L, 325 K. \n",
    "train = dataset[0:240]\n",
    "test = dataset[240:250]\n",
    "scaler = StandardScaler().fit(train.reshape(-1, train.shape[-1]))\n",
    "train_scaled = scaler.transform(train.reshape(-1, train.shape[-1])).reshape(train.shape)\n",
    "test_scaled = scaler.transform(test.reshape(-1, test.shape[-1])).reshape(test.shape)\n",
    "train_scaled_fs = np.array(np.split(train_scaled, 5, 2)).transpose(0,1,3,2) # 5 B, 240 N, 65 K, 200 L.\n",
    "\n",
    "\n",
    "fs_batch1 = train_scaled_fs[:,0:40,:,:]\n",
    "fs_batch2 = train_scaled_fs[:,40:80,:,:]\n",
    "fs_batch3 = train_scaled_fs[:,80:120,:,:]\n",
    "fs_batch4 = train_scaled_fs[:,120:160,:,:]\n",
    "fs_batch5 = train_scaled_fs[:,160:200,:,:]\n",
    "fs_batch6 = train_scaled_fs[:,160:240,:,:]\n",
    "\n",
    "\n",
    "# training train.py\n",
    "n_iter = ckpt_iter + 1\n",
    "while n_iter < n_iters + 1:\n",
    "    for batch_fs in zip(fs_batch1,fs_batch2,fs_batch3,fs_batch4,fs_batch5,fs_batch6): # 5 B, 40 N, 65 K, 200 L \n",
    "        for batch in batch_d: # 40 N, 65 K, 200 L  (batch into the model)\n",
    "            \n",
    "            \n",
    "# same principle for testing/inference.\n",
    "\n",
    "# to compute metrics: \n",
    "# 0) concatenate all batches if want to compute all together, oterwise compute metric per batch\n",
    "# 1) transpose back the data to N, L, K: if numpy use transpose and concatenate\n",
    "# 2) inverse transform the scaled data\n",
    "# 3) compute metrics... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3891c704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
